"""
æ™ºèƒ½è°ƒä¼˜ç³»ç»Ÿ
è‡ªåŠ¨ä¼˜åŒ–è¶…å‚æ•°ã€æ¨¡å‹é€‰æ‹©ã€ç‰¹å¾å·¥ç¨‹ç­‰
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Any, Optional, Tuple, Callable, Union
import json
import time
import logging
from dataclasses import dataclass, asdict
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, as_completed
import itertools
import random
from abc import ABC, abstractmethod

# ä¼˜åŒ–ç®—æ³•å¯¼å…¥
try:
    from scipy.optimize import minimize, differential_evolution, basinhopping
    from scipy.stats import uniform, randint
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

try:
    from sklearn.model_selection import cross_val_score, ParameterGrid, RandomizedSearchCV
    from sklearn.metrics import mean_squared_error, accuracy_score, f1_score
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

logger = logging.getLogger(__name__)


class OptimizationMethod(Enum):
    """ä¼˜åŒ–æ–¹æ³•"""
    GRID_SEARCH = "grid_search"
    RANDOM_SEARCH = "random_search"
    BAYESIAN = "bayesian"
    GENETIC = "genetic"
    PARTICLE_SWARM = "particle_swarm"
    SIMULATED_ANNEALING = "simulated_annealing"
    DIFFERENTIAL_EVOLUTION = "differential_evolution"
    OPTUNA = "optuna"


class OptimizationObjective(Enum):
    """ä¼˜åŒ–ç›®æ ‡"""
    ACCURACY = "accuracy"
    PRECISION = "precision"
    RECALL = "recall"
    F1_SCORE = "f1_score"
    ROC_AUC = "roc_auc"
    MSE = "mse"
    RMSE = "rmse"
    MAE = "mae"
    R2_SCORE = "r2_score"
    CUSTOM = "custom"


@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    best_params: Dict[str, Any]
    best_score: float
    optimization_history: List[Dict[str, Any]]
    method: str
    objective: str
    total_evaluations: int
    optimization_time: float
    convergence_info: Dict[str, Any]


@dataclass
class ParameterSpace:
    """å‚æ•°ç©ºé—´å®šä¹‰"""
    name: str
    param_type: str  # 'int', 'float', 'categorical', 'bool'
    bounds: Optional[Tuple[Any, Any]] = None  # (min, max) for numeric
    choices: Optional[List[Any]] = None  # for categorical
    log_scale: bool = False  # æ˜¯å¦ä½¿ç”¨å¯¹æ•°å°ºåº¦


class BaseOptimizer(ABC):
    """ä¼˜åŒ–å™¨åŸºç±»"""
    
    def __init__(self, objective_function: Callable, parameter_spaces: List[ParameterSpace]):
        self.objective_function = objective_function
        self.parameter_spaces = parameter_spaces
        self.optimization_history = []
        self.best_params = None
        self.best_score = float('-inf')
        self.evaluation_count = 0
    
    @abstractmethod
    def optimize(self, max_evaluations: int = 100, **kwargs) -> OptimizationResult:
        """æ‰§è¡Œä¼˜åŒ–"""
        pass
    
    def _evaluate(self, params: Dict[str, Any]) -> float:
        """è¯„ä¼°å‚æ•°ç»„åˆ"""
        try:
            score = self.objective_function(params)
            self.evaluation_count += 1
            
            # è®°å½•å†å²
            self.optimization_history.append({
                'evaluation': self.evaluation_count,
                'params': params.copy(),
                'score': score,
                'timestamp': time.time()
            })
            
            # æ›´æ–°æœ€ä½³ç»“æœ
            if score > self.best_score:
                self.best_score = score
                self.best_params = params.copy()
            
            return score
            
        except Exception as e:
            logger.error(f"å‚æ•°è¯„ä¼°å¤±è´¥: {e}")
            return float('-inf')
    
    def _generate_random_params(self) -> Dict[str, Any]:
        """ç”Ÿæˆéšæœºå‚æ•°"""
        params = {}
        
        for space in self.parameter_spaces:
            if space.param_type == 'int':
                if space.log_scale:
                    log_min = np.log10(space.bounds[0])
                    log_max = np.log10(space.bounds[1])
                    value = int(10 ** np.random.uniform(log_min, log_max))
                else:
                    value = np.random.randint(space.bounds[0], space.bounds[1] + 1)
                params[space.name] = value
                
            elif space.param_type == 'float':
                if space.log_scale:
                    log_min = np.log10(space.bounds[0])
                    log_max = np.log10(space.bounds[1])
                    value = 10 ** np.random.uniform(log_min, log_max)
                else:
                    value = np.random.uniform(space.bounds[0], space.bounds[1])
                params[space.name] = value
                
            elif space.param_type == 'categorical':
                params[space.name] = np.random.choice(space.choices)
                
            elif space.param_type == 'bool':
                params[space.name] = np.random.choice([True, False])
        
        return params


class GridSearchOptimizer(BaseOptimizer):
    """ç½‘æ ¼æœç´¢ä¼˜åŒ–å™¨"""
    
    def optimize(self, max_evaluations: int = 100, **kwargs) -> OptimizationResult:
        """æ‰§è¡Œç½‘æ ¼æœç´¢"""
        start_time = time.time()
        
        # ç”Ÿæˆå‚æ•°ç½‘æ ¼
        param_grid = {}
        for space in self.parameter_spaces:
            if space.param_type == 'int':
                step = max(1, (space.bounds[1] - space.bounds[0]) // 10)
                param_grid[space.name] = list(range(space.bounds[0], space.bounds[1] + 1, step))
            elif space.param_type == 'float':
                param_grid[space.name] = np.linspace(space.bounds[0], space.bounds[1], 10).tolist()
            elif space.param_type == 'categorical':
                param_grid[space.name] = space.choices
            elif space.param_type == 'bool':
                param_grid[space.name] = [True, False]
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        keys = param_grid.keys()
        values = param_grid.values()
        param_combinations = [dict(zip(keys, combination)) for combination in itertools.product(*values)]
        
        # é™åˆ¶è¯„ä¼°æ¬¡æ•°
        if len(param_combinations) > max_evaluations:
            param_combinations = random.sample(param_combinations, max_evaluations)
        
        # è¯„ä¼°æ‰€æœ‰ç»„åˆ
        for params in param_combinations:
            self._evaluate(params)
        
        optimization_time = time.time() - start_time
        
        return OptimizationResult(
            best_params=self.best_params,
            best_score=self.best_score,
            optimization_history=self.optimization_history,
            method="grid_search",
            objective="custom",
            total_evaluations=self.evaluation_count,
            optimization_time=optimization_time,
            convergence_info={'total_combinations': len(param_combinations)}
        )


class RandomSearchOptimizer(BaseOptimizer):
    """éšæœºæœç´¢ä¼˜åŒ–å™¨"""
    
    def optimize(self, max_evaluations: int = 100, **kwargs) -> OptimizationResult:
        """æ‰§è¡Œéšæœºæœç´¢"""
        start_time = time.time()
        
        for _ in range(max_evaluations):
            params = self._generate_random_params()
            self._evaluate(params)
        
        optimization_time = time.time() - start_time
        
        return OptimizationResult(
            best_params=self.best_params,
            best_score=self.best_score,
            optimization_history=self.optimization_history,
            method="random_search",
            objective="custom",
            total_evaluations=self.evaluation_count,
            optimization_time=optimization_time,
            convergence_info={'max_evaluations': max_evaluations}
        )


class BayesianOptimizer(BaseOptimizer):
    """è´å¶æ–¯ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨Optunaï¼‰"""
    
    def optimize(self, max_evaluations: int = 100, **kwargs) -> OptimizationResult:
        """æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–"""
        if not OPTUNA_AVAILABLE:
            logger.error("Optunaä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œè´å¶æ–¯ä¼˜åŒ–")
            return self._fallback_to_random_search(max_evaluations)
        
        start_time = time.time()
        
        def objective(trial):
            params = {}
            for space in self.parameter_spaces:
                if space.param_type == 'int':
                    if space.log_scale:
                        params[space.name] = trial.suggest_int(
                            space.name, space.bounds[0], space.bounds[1], log=True
                        )
                    else:
                        params[space.name] = trial.suggest_int(
                            space.name, space.bounds[0], space.bounds[1]
                        )
                elif space.param_type == 'float':
                    if space.log_scale:
                        params[space.name] = trial.suggest_float(
                            space.name, space.bounds[0], space.bounds[1], log=True
                        )
                    else:
                        params[space.name] = trial.suggest_float(
                            space.name, space.bounds[0], space.bounds[1]
                        )
                elif space.param_type == 'categorical':
                    params[space.name] = trial.suggest_categorical(space.name, space.choices)
                elif space.param_type == 'bool':
                    params[space.name] = trial.suggest_categorical(space.name, [True, False])
            
            return self._evaluate(params)
        
        # åˆ›å»ºç ”ç©¶
        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=max_evaluations)
        
        optimization_time = time.time() - start_time
        
        return OptimizationResult(
            best_params=study.best_params,
            best_score=study.best_value,
            optimization_history=self.optimization_history,
            method="bayesian",
            objective="custom",
            total_evaluations=len(study.trials),
            optimization_time=optimization_time,
            convergence_info={
                'n_trials': len(study.trials),
                'best_trial': study.best_trial.number
            }
        )
    
    def _fallback_to_random_search(self, max_evaluations: int) -> OptimizationResult:
        """å›é€€åˆ°éšæœºæœç´¢"""
        logger.warning("å›é€€åˆ°éšæœºæœç´¢ä¼˜åŒ–")
        random_optimizer = RandomSearchOptimizer(self.objective_function, self.parameter_spaces)
        return random_optimizer.optimize(max_evaluations)


class GeneticOptimizer(BaseOptimizer):
    """é—ä¼ ç®—æ³•ä¼˜åŒ–å™¨"""
    
    def optimize(self, max_evaluations: int = 100, population_size: int = 20, 
                mutation_rate: float = 0.1, crossover_rate: float = 0.8, **kwargs) -> OptimizationResult:
        """æ‰§è¡Œé—ä¼ ç®—æ³•ä¼˜åŒ–"""
        start_time = time.time()
        
        # åˆå§‹åŒ–ç§ç¾¤
        population = [self._generate_random_params() for _ in range(population_size)]
        fitness_scores = [self._evaluate(params) for params in population]
        
        generations = max_evaluations // population_size
        
        for generation in range(generations):
            # é€‰æ‹©
            selected = self._selection(population, fitness_scores, population_size // 2)
            
            # äº¤å‰å’Œå˜å¼‚
            offspring = []
            for i in range(0, len(selected), 2):
                parent1 = selected[i]
                parent2 = selected[i + 1] if i + 1 < len(selected) else selected[0]
                
                if random.random() < crossover_rate:
                    child1, child2 = self._crossover(parent1, parent2)
                else:
                    child1, child2 = parent1.copy(), parent2.copy()
                
                if random.random() < mutation_rate:
                    child1 = self._mutate(child1)
                if random.random() < mutation_rate:
                    child2 = self._mutate(child2)
                
                offspring.extend([child1, child2])
            
            # è¯„ä¼°åä»£
            offspring_fitness = [self._evaluate(params) for params in offspring]
            
            # ç”Ÿæˆæ–°ä¸€ä»£
            all_individuals = population + offspring
            all_fitness = fitness_scores + offspring_fitness
            
            # é€‰æ‹©æœ€ä½³ä¸ªä½“
            sorted_indices = sorted(range(len(all_fitness)), key=lambda i: all_fitness[i], reverse=True)
            population = [all_individuals[i] for i in sorted_indices[:population_size]]
            fitness_scores = [all_fitness[i] for i in sorted_indices[:population_size]]
        
        optimization_time = time.time() - start_time
        
        return OptimizationResult(
            best_params=self.best_params,
            best_score=self.best_score,
            optimization_history=self.optimization_history,
            method="genetic",
            objective="custom",
            total_evaluations=self.evaluation_count,
            optimization_time=optimization_time,
            convergence_info={
                'generations': generations,
                'population_size': population_size,
                'final_best_fitness': max(fitness_scores)
            }
        )
    
    def _selection(self, population: List[Dict], fitness_scores: List[float], num_selected: int) -> List[Dict]:
        """é€‰æ‹©æ“ä½œï¼ˆé”¦æ ‡èµ›é€‰æ‹©ï¼‰"""
        selected = []
        tournament_size = 3
        
        for _ in range(num_selected):
            tournament_indices = random.sample(range(len(population)), min(tournament_size, len(population)))
            winner_idx = max(tournament_indices, key=lambda i: fitness_scores[i])
            selected.append(population[winner_idx].copy())
        
        return selected
    
    def _crossover(self, parent1: Dict, parent2: Dict) -> Tuple[Dict, Dict]:
        """äº¤å‰æ“ä½œ"""
        child1 = parent1.copy()
        child2 = parent2.copy()
        
        # éšæœºé€‰æ‹©ä¸€äº›å‚æ•°è¿›è¡Œäº¤æ¢
        for space in self.parameter_spaces:
            if random.random() < 0.5:
                child1[space.name], child2[space.name] = child2[space.name], child1[space.name]
        
        return child1, child2
    
    def _mutate(self, individual: Dict) -> Dict:
        """å˜å¼‚æ“ä½œ"""
        mutated = individual.copy()
        
        # éšæœºå˜å¼‚ä¸€äº›å‚æ•°
        for space in self.parameter_spaces:
            if random.random() < 0.3:  # 30%çš„å‚æ•°å˜å¼‚æ¦‚ç‡
                if space.param_type == 'int':
                    # åœ¨åŸå€¼é™„è¿‘å˜å¼‚
                    current_val = mutated[space.name]
                    mutation_range = max(1, (space.bounds[1] - space.bounds[0]) // 10)
                    new_val = current_val + random.randint(-mutation_range, mutation_range)
                    mutated[space.name] = max(space.bounds[0], min(space.bounds[1], new_val))
                    
                elif space.param_type == 'float':
                    # åœ¨åŸå€¼é™„è¿‘å˜å¼‚
                    current_val = mutated[space.name]
                    mutation_range = (space.bounds[1] - space.bounds[0]) * 0.1
                    new_val = current_val + random.uniform(-mutation_range, mutation_range)
                    mutated[space.name] = max(space.bounds[0], min(space.bounds[1], new_val))
                    
                elif space.param_type == 'categorical':
                    mutated[space.name] = random.choice(space.choices)
                    
                elif space.param_type == 'bool':
                    mutated[space.name] = not mutated[space.name]
        
        return mutated


class ParticleSwarmOptimizer(BaseOptimizer):
    """ç²’å­ç¾¤ä¼˜åŒ–å™¨"""
    
    def optimize(self, max_evaluations: int = 100, swarm_size: int = 20, 
                w: float = 0.5, c1: float = 1.5, c2: float = 1.5, **kwargs) -> OptimizationResult:
        """æ‰§è¡Œç²’å­ç¾¤ä¼˜åŒ–"""
        start_time = time.time()
        
        # åˆå§‹åŒ–ç²’å­ç¾¤
        particles = []
        for _ in range(swarm_size):
            position = self._generate_random_params()
            velocity = {}
            
            # åˆå§‹åŒ–é€Ÿåº¦
            for space in self.parameter_spaces:
                if space.param_type in ['int', 'float']:
                    velocity[space.name] = 0.0
                else:
                    velocity[space.name] = None
            
            particles.append({
                'position': position,
                'velocity': velocity,
                'best_position': position.copy(),
                'best_score': self._evaluate(position)
            })
        
        # å…¨å±€æœ€ä½³ä½ç½®
        global_best_particle = max(particles, key=lambda p: p['best_score'])
        global_best_position = global_best_particle['best_position'].copy()
        global_best_score = global_best_particle['best_score']
        
        iterations = max_evaluations // swarm_size
        
        for iteration in range(iterations):
            for particle in particles:
                # æ›´æ–°é€Ÿåº¦å’Œä½ç½®
                self._update_particle(particle, global_best_position, w, c1, c2)
                
                # è¯„ä¼°æ–°ä½ç½®
                score = self._evaluate(particle['position'])
                
                # æ›´æ–°ä¸ªä½“æœ€ä½³
                if score > particle['best_score']:
                    particle['best_score'] = score
                    particle['best_position'] = particle['position'].copy()
                
                # æ›´æ–°å…¨å±€æœ€ä½³
                if score > global_best_score:
                    global_best_score = score
                    global_best_position = particle['position'].copy()
        
        optimization_time = time.time() - start_time
        
        return OptimizationResult(
            best_params=global_best_position,
            best_score=global_best_score,
            optimization_history=self.optimization_history,
            method="particle_swarm",
            objective="custom",
            total_evaluations=self.evaluation_count,
            optimization_time=optimization_time,
            convergence_info={
                'iterations': iterations,
                'swarm_size': swarm_size,
                'final_global_best': global_best_score
            }
        )
    
    def _update_particle(self, particle: Dict, global_best_position: Dict, 
                        w: float, c1: float, c2: float):
        """æ›´æ–°ç²’å­ä½ç½®å’Œé€Ÿåº¦"""
        for space in self.parameter_spaces:
            if space.param_type in ['int', 'float']:
                # æ›´æ–°é€Ÿåº¦
                r1, r2 = random.random(), random.random()
                
                cognitive = c1 * r1 * (particle['best_position'][space.name] - particle['position'][space.name])
                social = c2 * r2 * (global_best_position[space.name] - particle['position'][space.name])
                
                particle['velocity'][space.name] = (
                    w * particle['velocity'][space.name] + cognitive + social
                )
                
                # æ›´æ–°ä½ç½®
                new_position = particle['position'][space.name] + particle['velocity'][space.name]
                
                # è¾¹ç•Œå¤„ç†
                if space.param_type == 'int':
                    new_position = int(round(new_position))
                    new_position = max(space.bounds[0], min(space.bounds[1], new_position))
                else:  # float
                    new_position = max(space.bounds[0], min(space.bounds[1], new_position))
                
                particle['position'][space.name] = new_position
            
            else:  # categorical or bool
                # å¯¹äºç¦»æ•£å˜é‡ï¼Œéšæœºé€‰æ‹©
                if random.random() < 0.1:  # 10%çš„æ¦‚ç‡æ”¹å˜
                    if space.param_type == 'categorical':
                        particle['position'][space.name] = random.choice(space.choices)
                    else:  # bool
                        particle['position'][space.name] = not particle['position'][space.name]


class IntelligentTuner:
    """æ™ºèƒ½è°ƒä¼˜ä¸»ç±»"""
    
    def __init__(self):
        self.optimizers = {
            OptimizationMethod.GRID_SEARCH: GridSearchOptimizer,
            OptimizationMethod.RANDOM_SEARCH: RandomSearchOptimizer,
            OptimizationMethod.BAYESIAN: BayesianOptimizer,
            OptimizationMethod.GENETIC: GeneticOptimizer,
            OptimizationMethod.PARTICLE_SWARM: ParticleSwarmOptimizer
        }
        
        self.optimization_history = []
        self.best_configurations = {}
        
        logger.info("æ™ºèƒ½è°ƒä¼˜ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def optimize_model_parameters(self, model_class, parameter_spaces: List[ParameterSpace],
                                 X_train, y_train, X_val, y_val,
                                 method: OptimizationMethod = OptimizationMethod.BAYESIAN,
                                 max_evaluations: int = 50,
                                 objective: OptimizationObjective = OptimizationObjective.ACCURACY,
                                 **kwargs) -> OptimizationResult:
        """ä¼˜åŒ–æ¨¡å‹å‚æ•°"""
        
        def objective_function(params: Dict[str, Any]) -> float:
            """ç›®æ ‡å‡½æ•°"""
            try:
                # åˆ›å»ºæ¨¡å‹å®ä¾‹
                model = model_class(**params)
                
                # è®­ç»ƒæ¨¡å‹
                model.fit(X_train, y_train)
                
                # é¢„æµ‹
                y_pred = model.predict(X_val)
                
                # è®¡ç®—è¯„åˆ†
                if objective == OptimizationObjective.ACCURACY:
                    score = accuracy_score(y_val, y_pred)
                elif objective == OptimizationObjective.MSE:
                    score = -mean_squared_error(y_val, y_pred)  # è´Ÿå·å› ä¸ºè¦æœ€å¤§åŒ–
                elif objective == OptimizationObjective.RMSE:
                    score = -np.sqrt(mean_squared_error(y_val, y_pred))
                elif objective == OptimizationObjective.F1_SCORE:
                    score = f1_score(y_val, y_pred, average='weighted')
                else:
                    score = accuracy_score(y_val, y_pred)  # é»˜è®¤
                
                return score
                
            except Exception as e:
                logger.error(f"æ¨¡å‹è¯„ä¼°å¤±è´¥: {e}")
                return float('-inf')
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer_class = self.optimizers.get(method, RandomSearchOptimizer)
        optimizer = optimizer_class(objective_function, parameter_spaces)
        
        # æ‰§è¡Œä¼˜åŒ–
        result = optimizer.optimize(max_evaluations, **kwargs)
        
        # è®°å½•å†å²
        self.optimization_history.append({
            'timestamp': time.time(),
            'method': method.value,
            'objective': objective.value,
            'result': result
        })
        
        return result
    
    def auto_feature_selection(self, X: pd.DataFrame, y: pd.Series,
                              feature_selection_methods: List[str] = None,
                              max_features: int = None) -> Dict[str, Any]:
        """è‡ªåŠ¨ç‰¹å¾é€‰æ‹©"""
        if not SKLEARN_AVAILABLE:
            logger.error("sklearnä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œç‰¹å¾é€‰æ‹©")
            return {'selected_features': list(X.columns)}
        
        from sklearn.feature_selection import SelectKBest, RFE, SelectFromModel
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.linear_model import LassoCV
        
        feature_selection_methods = feature_selection_methods or ['univariate', 'rfe', 'lasso']
        max_features = max_features or min(20, X.shape[1])
        
        results = {}
        
        try:
            # å•å˜é‡ç‰¹å¾é€‰æ‹©
            if 'univariate' in feature_selection_methods:
                selector = SelectKBest(k=max_features)
                X_selected = selector.fit_transform(X, y)
                selected_features = X.columns[selector.get_support()].tolist()
                results['univariate'] = {
                    'features': selected_features,
                    'scores': selector.scores_.tolist()
                }
            
            # é€’å½’ç‰¹å¾æ¶ˆé™¤
            if 'rfe' in feature_selection_methods:
                estimator = RandomForestRegressor(n_estimators=10, random_state=42)
                selector = RFE(estimator, n_features_to_select=max_features)
                selector.fit(X, y)
                selected_features = X.columns[selector.support_].tolist()
                results['rfe'] = {
                    'features': selected_features,
                    'ranking': selector.ranking_.tolist()
                }
            
            # åŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©ï¼ˆLassoï¼‰
            if 'lasso' in feature_selection_methods:
                lasso = LassoCV(cv=5, random_state=42)
                lasso.fit(X, y)
                selector = SelectFromModel(lasso, prefit=True, max_features=max_features)
                selected_features = X.columns[selector.get_support()].tolist()
                results['lasso'] = {
                    'features': selected_features,
                    'coefficients': lasso.coef_.tolist()
                }
            
            # ç»¼åˆç‰¹å¾é€‰æ‹©
            all_features = set()
            for method_result in results.values():
                all_features.update(method_result['features'])
            
            # æŒ‰å‡ºç°é¢‘ç‡æ’åº
            feature_counts = {}
            for method_result in results.values():
                for feature in method_result['features']:
                    feature_counts[feature] = feature_counts.get(feature, 0) + 1
            
            sorted_features = sorted(feature_counts.items(), key=lambda x: x[1], reverse=True)
            final_features = [f for f, _ in sorted_features[:max_features]]
            
            results['final_selection'] = {
                'features': final_features,
                'feature_counts': feature_counts
            }
            
            logger.info(f"ç‰¹å¾é€‰æ‹©å®Œæˆï¼Œä»{X.shape[1]}ä¸ªç‰¹å¾ä¸­é€‰æ‹©äº†{len(final_features)}ä¸ª")
            
        except Exception as e:
            logger.error(f"ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
            results['final_selection'] = {'features': list(X.columns)}
        
        return results
    
    def optimize_ensemble_weights(self, predictions: Dict[str, np.ndarray], 
                                 y_true: np.ndarray,
                                 method: OptimizationMethod = OptimizationMethod.BAYESIAN) -> Dict[str, float]:
        """ä¼˜åŒ–é›†æˆæ¨¡å‹æƒé‡"""
        model_names = list(predictions.keys())
        n_models = len(model_names)
        
        if n_models < 2:
            return {model_names[0]: 1.0} if model_names else {}
        
        # å®šä¹‰å‚æ•°ç©ºé—´
        parameter_spaces = []
        for i, model_name in enumerate(model_names):
            parameter_spaces.append(ParameterSpace(
                name=f"weight_{model_name}",
                param_type='float',
                bounds=(0.0, 1.0)
            ))
        
        def objective_function(params: Dict[str, Any]) -> float:
            """é›†æˆç›®æ ‡å‡½æ•°"""
            try:
                # æ ‡å‡†åŒ–æƒé‡
                weights = np.array([params[f"weight_{name}"] for name in model_names])
                weights = weights / np.sum(weights)  # å½’ä¸€åŒ–
                
                # è®¡ç®—åŠ æƒé¢„æµ‹
                ensemble_pred = np.zeros_like(y_true, dtype=float)
                for i, model_name in enumerate(model_names):
                    ensemble_pred += weights[i] * predictions[model_name]
                
                # è®¡ç®—è¯„åˆ†ï¼ˆè´ŸMSEï¼Œå› ä¸ºè¦æœ€å¤§åŒ–ï¼‰
                score = -mean_squared_error(y_true, ensemble_pred)
                return score
                
            except Exception as e:
                logger.error(f"é›†æˆæƒé‡è¯„ä¼°å¤±è´¥: {e}")
                return float('-inf')
        
        # æ‰§è¡Œä¼˜åŒ–
        optimizer_class = self.optimizers.get(method, RandomSearchOptimizer)
        optimizer = optimizer_class(objective_function, parameter_spaces)
        result = optimizer.optimize(max_evaluations=50)
        
        # æå–å¹¶æ ‡å‡†åŒ–æƒé‡
        raw_weights = {}
        for model_name in model_names:
            raw_weights[model_name] = result.best_params[f"weight_{model_name}"]
        
        # æ ‡å‡†åŒ–æƒé‡
        total_weight = sum(raw_weights.values())
        normalized_weights = {name: weight / total_weight for name, weight in raw_weights.items()}
        
        logger.info(f"é›†æˆæƒé‡ä¼˜åŒ–å®Œæˆ: {normalized_weights}")
        return normalized_weights
    
    def adaptive_learning_rate_schedule(self, model, X_train, y_train, X_val, y_val,
                                      initial_lr: float = 0.01, patience: int = 10) -> List[float]:
        """è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦"""
        learning_rates = [initial_lr]
        best_score = float('-inf')
        patience_counter = 0
        
        current_lr = initial_lr
        
        for epoch in range(100):  # æœ€å¤š100ä¸ªepoch
            try:
                # è®¾ç½®å­¦ä¹ ç‡ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
                if hasattr(model, 'set_params'):
                    model.set_params(learning_rate=current_lr)
                elif hasattr(model, 'learning_rate'):
                    model.learning_rate = current_lr
                
                # è®­ç»ƒä¸€ä¸ªepochï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
                if hasattr(model, 'partial_fit'):
                    model.partial_fit(X_train, y_train)
                else:
                    model.fit(X_train, y_train)
                
                # éªŒè¯
                y_pred = model.predict(X_val)
                score = -mean_squared_error(y_val, y_pred)
                
                # æ£€æŸ¥æ”¹è¿›
                if score > best_score:
                    best_score = score
                    patience_counter = 0
                else:
                    patience_counter += 1
                
                # æ—©åœæˆ–è°ƒæ•´å­¦ä¹ ç‡
                if patience_counter >= patience:
                    if current_lr > 1e-6:
                        current_lr *= 0.5  # å‡åŠå­¦ä¹ ç‡
                        patience_counter = 0
                        logger.info(f"å­¦ä¹ ç‡è°ƒæ•´ä¸º: {current_lr}")
                    else:
                        logger.info("å­¦ä¹ ç‡å·²è¾¾åˆ°æœ€å°å€¼ï¼Œåœæ­¢è®­ç»ƒ")
                        break
                
                learning_rates.append(current_lr)
                
            except Exception as e:
                logger.error(f"è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒåº¦å¤±è´¥: {e}")
                break
        
        return learning_rates
    
    def get_optimization_summary(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æ€»ç»“"""
        if not self.optimization_history:
            return {'message': 'æš‚æ— ä¼˜åŒ–å†å²'}
        
        summary = {
            'total_optimizations': len(self.optimization_history),
            'methods_used': {},
            'objectives_used': {},
            'best_results': {},
            'average_optimization_time': 0
        }
        
        total_time = 0
        
        for opt in self.optimization_history:
            # ç»Ÿè®¡æ–¹æ³•ä½¿ç”¨
            method = opt['method']
            summary['methods_used'][method] = summary['methods_used'].get(method, 0) + 1
            
            # ç»Ÿè®¡ç›®æ ‡ä½¿ç”¨
            objective = opt['objective']
            summary['objectives_used'][objective] = summary['objectives_used'].get(objective, 0) + 1
            
            # è®°å½•æœ€ä½³ç»“æœ
            key = f"{method}_{objective}"
            if key not in summary['best_results'] or opt['result'].best_score > summary['best_results'][key]['score']:
                summary['best_results'][key] = {
                    'score': opt['result'].best_score,
                    'params': opt['result'].best_params,
                    'evaluations': opt['result'].total_evaluations
                }
            
            total_time += opt['result'].optimization_time
        
        summary['average_optimization_time'] = total_time / len(self.optimization_history)
        
        return summary


# å…¨å±€æ™ºèƒ½è°ƒä¼˜å™¨å®ä¾‹
_intelligent_tuner = None

def get_intelligent_tuner() -> IntelligentTuner:
    """è·å–æ™ºèƒ½è°ƒä¼˜å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _intelligent_tuner
    if _intelligent_tuner is None:
        _intelligent_tuner = IntelligentTuner()
    return _intelligent_tuner


def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    # åˆ›å»ºæ™ºèƒ½è°ƒä¼˜å™¨
    tuner = get_intelligent_tuner()
    
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    np.random.seed(42)
    X = pd.DataFrame(np.random.randn(1000, 10), columns=[f'feature_{i}' for i in range(10)])
    y = pd.Series(np.random.randn(1000))
    
    # åˆ†å‰²æ•°æ®
    split_idx = 800
    X_train, X_val = X[:split_idx], X[split_idx:]
    y_train, y_val = y[:split_idx], y[split_idx:]
    
    print("ğŸ§  æµ‹è¯•æ™ºèƒ½è°ƒä¼˜ç³»ç»Ÿ...")
    
    # æµ‹è¯•ç‰¹å¾é€‰æ‹©
    print("\n1. è‡ªåŠ¨ç‰¹å¾é€‰æ‹©æµ‹è¯•...")
    try:
        feature_results = tuner.auto_feature_selection(X_train, y_train, max_features=5)
        selected_features = feature_results['final_selection']['features']
        print(f"   é€‰æ‹©çš„ç‰¹å¾: {selected_features}")
    except Exception as e:
        print(f"   ç‰¹å¾é€‰æ‹©æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•å‚æ•°ä¼˜åŒ–ï¼ˆä½¿ç”¨ç®€å•çš„çº¿æ€§æ¨¡å‹ï¼‰
    print("\n2. å‚æ•°ä¼˜åŒ–æµ‹è¯•...")
    try:
        if SKLEARN_AVAILABLE:
            from sklearn.linear_model import Ridge
            
            # å®šä¹‰å‚æ•°ç©ºé—´
            param_spaces = [
                ParameterSpace('alpha', 'float', bounds=(0.01, 10.0), log_scale=True)
            ]
            
            # æ‰§è¡Œä¼˜åŒ–
            result = tuner.optimize_model_parameters(
                Ridge, param_spaces, X_train, y_train, X_val, y_val,
                method=OptimizationMethod.RANDOM_SEARCH,
                max_evaluations=10,
                objective=OptimizationObjective.MSE
            )
            
            print(f"   æœ€ä½³å‚æ•°: {result.best_params}")
            print(f"   æœ€ä½³åˆ†æ•°: {result.best_score:.4f}")
            print(f"   ä¼˜åŒ–æ—¶é—´: {result.optimization_time:.2f}ç§’")
        else:
            print("   è·³è¿‡å‚æ•°ä¼˜åŒ–æµ‹è¯•ï¼ˆsklearnä¸å¯ç”¨ï¼‰")
    except Exception as e:
        print(f"   å‚æ•°ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•é›†æˆæƒé‡ä¼˜åŒ–
    print("\n3. é›†æˆæƒé‡ä¼˜åŒ–æµ‹è¯•...")
    try:
        # æ¨¡æ‹Ÿå¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
        predictions = {
            'model_1': np.random.randn(len(y_val)),
            'model_2': np.random.randn(len(y_val)),
            'model_3': np.random.randn(len(y_val))
        }
        
        weights = tuner.optimize_ensemble_weights(
            predictions, y_val.values,
            method=OptimizationMethod.RANDOM_SEARCH
        )
        
        print(f"   ä¼˜åŒ–çš„æƒé‡: {weights}")
    except Exception as e:
        print(f"   é›†æˆæƒé‡ä¼˜åŒ–æµ‹è¯•å¤±è´¥: {e}")
    
    # è¾“å‡ºä¼˜åŒ–æ€»ç»“
    print("\n4. ä¼˜åŒ–æ€»ç»“...")
    summary = tuner.get_optimization_summary()
    print(json.dumps(summary, indent=2, ensure_ascii=False))
    
    print("\nâœ… æ™ºèƒ½è°ƒä¼˜ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
