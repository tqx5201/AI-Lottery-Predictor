"""
AIæ™ºèƒ½åŠ©æ‰‹ç³»ç»Ÿ
æä¾›è‡ªç„¶è¯­è¨€äº¤äº’ã€æ™ºèƒ½åˆ†æã€å†³ç­–æ”¯æŒç­‰åŠŸèƒ½
"""

import json
import time
import re
import logging
from typing import Dict, List, Any, Optional, Tuple, Callable, Union
from dataclasses import dataclass, asdict
from enum import Enum
from datetime import datetime, timedelta
import numpy as np
import pandas as pd
from abc import ABC, abstractmethod

# NLPå’ŒAIåº“å¯¼å…¥
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    openai = None

try:
    import torch
    from transformers import pipeline, AutoTokenizer, AutoModel
    TRANSFORMERS_AVAILABLE = True
    TORCH_AVAILABLE = True
except ImportError as e:
    TRANSFORMERS_AVAILABLE = False
    TORCH_AVAILABLE = False
    pipeline = AutoTokenizer = AutoModel = torch = None
    logger.warning(f"Transformers/PyTorchå¯¼å…¥å¤±è´¥: {e}")

try:
    import nltk
    from nltk.tokenize import word_tokenize, sent_tokenize
    from nltk.corpus import stopwords
    from nltk.stem import WordNetLemmatizer
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False
    nltk = word_tokenize = sent_tokenize = stopwords = WordNetLemmatizer = None

try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    spacy = None

logger = logging.getLogger(__name__)


class ConversationRole(Enum):
    """å¯¹è¯è§’è‰²"""
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"


class IntentType(Enum):
    """æ„å›¾ç±»å‹"""
    ANALYSIS_REQUEST = "analysis_request"
    PREDICTION_REQUEST = "prediction_request"
    DATA_QUERY = "data_query"
    HELP_REQUEST = "help_request"
    CONFIGURATION = "configuration"
    EXPLANATION = "explanation"
    RECOMMENDATION = "recommendation"
    COMPARISON = "comparison"
    UNKNOWN = "unknown"


class ResponseType(Enum):
    """å“åº”ç±»å‹"""
    TEXT = "text"
    DATA = "data"
    CHART = "chart"
    ACTION = "action"
    MIXED = "mixed"


@dataclass
class ConversationMessage:
    """å¯¹è¯æ¶ˆæ¯"""
    role: ConversationRole
    content: str
    timestamp: float
    metadata: Optional[Dict[str, Any]] = None
    
    def to_dict(self) -> Dict[str, Any]:
        result = asdict(self)
        result['role'] = self.role.value
        return result


@dataclass
class UserIntent:
    """ç”¨æˆ·æ„å›¾"""
    intent_type: IntentType
    confidence: float
    entities: Dict[str, Any]
    parameters: Dict[str, Any]
    context: Dict[str, Any]


@dataclass
class AssistantResponse:
    """åŠ©æ‰‹å“åº”"""
    response_type: ResponseType
    content: str
    data: Optional[Dict[str, Any]] = None
    actions: Optional[List[str]] = None
    confidence: float = 1.0
    metadata: Optional[Dict[str, Any]] = None


class NLPProcessor:
    """è‡ªç„¶è¯­è¨€å¤„ç†å™¨"""
    
    def __init__(self):
        self.tokenizer = None
        self.lemmatizer = None
        self.nlp_model = None
        self._initialize_nlp()
    
    def _initialize_nlp(self):
        """åˆå§‹åŒ–NLPå·¥å…·"""
        try:
            if NLTK_AVAILABLE:
                # ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
                try:
                    nltk.data.find('tokenizers/punkt')
                except LookupError:
                    nltk.download('punkt')
                
                try:
                    nltk.data.find('corpora/stopwords')
                except LookupError:
                    nltk.download('stopwords')
                
                try:
                    nltk.data.find('corpora/wordnet')
                except LookupError:
                    nltk.download('wordnet')
                
                self.lemmatizer = WordNetLemmatizer()
                logger.info("NLTKåˆå§‹åŒ–å®Œæˆ")
            
            if SPACY_AVAILABLE:
                self.nlp_model = None
                # å°è¯•åŠ è½½ä¸åŒçš„SpaCyæ¨¡å‹ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
                models_to_try = [
                    ("zh_core_web_sm", "ä¸­æ–‡"),
                    ("en_core_web_sm", "è‹±æ–‡"),
                    ("en_core_web_md", "è‹±æ–‡-ä¸­ç­‰"),
                    ("en_core_web_lg", "è‹±æ–‡-å¤§å‹")
                ]
                
                for model_name, model_desc in models_to_try:
                    try:
                        self.nlp_model = spacy.load(model_name)
                        logger.info(f"SpaCy{model_desc}æ¨¡å‹({model_name})åŠ è½½å®Œæˆ")
                        break
                    except OSError:
                        continue
                
                if self.nlp_model is None:
                    logger.warning("SpaCyæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·è¿è¡Œ 'python setup_models.py' å®‰è£…æ¨¡å‹")
            else:
                logger.info("SpaCyä¸å¯ç”¨ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½")
            
            if TRANSFORMERS_AVAILABLE and TORCH_AVAILABLE:
                try:
                    # æ£€æŸ¥PyTorchç‰ˆæœ¬
                    if hasattr(torch, '__version__'):
                        torch_version = torch.__version__
                        logger.info(f"PyTorchç‰ˆæœ¬: {torch_version}")
                    
                    # ä½¿ç”¨æ›´è½»é‡çº§çš„æ¨¡å‹ï¼Œé¿å…ç½‘ç»œä¸‹è½½é—®é¢˜
                    self.sentiment_pipeline = pipeline("sentiment-analysis", 
                                                      model="distilbert-base-uncased-finetuned-sst-2-english",
                                                      return_all_scores=True)
                    logger.info("Transformersæƒ…æ„Ÿåˆ†ææ¨¡å‹åŠ è½½å®Œæˆ")
                except Exception as e:
                    logger.warning(f"Transformersæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                    self.sentiment_pipeline = None
            else:
                logger.info("Transformersæˆ–PyTorchä¸å¯ç”¨ï¼Œè·³è¿‡æ¨¡å‹åŠ è½½")
                self.sentiment_pipeline = None
            
        except Exception as e:
            logger.error(f"NLPåˆå§‹åŒ–å¤±è´¥: {e}")
    
    def preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)  # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        text = text.strip().lower()
        
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """åˆ†è¯"""
        if self.nlp_model:
            doc = self.nlp_model(text)
            return [token.text for token in doc if not token.is_punct and not token.is_space]
        elif NLTK_AVAILABLE:
            return word_tokenize(text)
        else:
            return text.split()
    
    def extract_entities(self, text: str) -> Dict[str, List[str]]:
        """æå–å®ä½“"""
        entities = {
            'numbers': [],
            'dates': [],
            'models': [],
            'actions': [],
            'periods': []
        }
        
        # æ•°å­—æå–
        numbers = re.findall(r'\d+', text)
        entities['numbers'] = numbers
        
        # æ—¥æœŸæå–
        date_patterns = [
            r'\d{4}[-/]\d{1,2}[-/]\d{1,2}',
            r'\d{1,2}[-/]\d{1,2}[-/]\d{4}',
            r'\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥'
        ]
        for pattern in date_patterns:
            dates = re.findall(pattern, text)
            entities['dates'].extend(dates)
        
        # æ¨¡å‹åç§°æå–
        model_keywords = ['éšæœºæ£®æ—', 'xgboost', 'lstm', 'ç¥ç»ç½‘ç»œ', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ']
        for keyword in model_keywords:
            if keyword in text:
                entities['models'].append(keyword)
        
        # åŠ¨ä½œæå–
        action_keywords = ['é¢„æµ‹', 'åˆ†æ', 'æŸ¥è¯¢', 'æ¯”è¾ƒ', 'æ¨è', 'è§£é‡Š', 'é…ç½®']
        for keyword in action_keywords:
            if keyword in text:
                entities['actions'].append(keyword)
        
        # æœŸå·æå–
        period_pattern = r'\d{4}\d{3}'
        periods = re.findall(period_pattern, text)
        entities['periods'] = periods
        
        return entities
    
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """æƒ…æ„Ÿåˆ†æ"""
        if hasattr(self, 'sentiment_pipeline') and self.sentiment_pipeline:
            try:
                result = self.sentiment_pipeline(text)
                return {
                    'sentiment': result[0]['label'],
                    'confidence': result[0]['score']
                }
            except Exception as e:
                logger.warning(f"æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
        
        # ç®€å•çš„è§„åˆ™åŸºç¡€æƒ…æ„Ÿåˆ†æ
        positive_words = ['å¥½', 'æ£’', 'ä¼˜ç§€', 'æ»¡æ„', 'å–œæ¬¢', 'èµ', 'ä¸é”™']
        negative_words = ['å·®', 'ç³Ÿ', 'å¤±æœ›', 'ä¸æ»¡', 'è®¨åŒ', 'çƒ‚', 'é—®é¢˜']
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            return {'sentiment': 'POSITIVE', 'confidence': 0.7}
        elif negative_count > positive_count:
            return {'sentiment': 'NEGATIVE', 'confidence': 0.7}
        else:
            return {'sentiment': 'NEUTRAL', 'confidence': 0.6}


class IntentClassifier:
    """æ„å›¾åˆ†ç±»å™¨"""
    
    def __init__(self):
        self.intent_patterns = self._build_intent_patterns()
    
    def _build_intent_patterns(self) -> Dict[IntentType, List[str]]:
        """æ„å»ºæ„å›¾æ¨¡å¼"""
        return {
            IntentType.ANALYSIS_REQUEST: [
                'åˆ†æ', 'ç»Ÿè®¡', 'è¶‹åŠ¿', 'é¢‘ç‡', 'è§„å¾‹', 'æ¨¡å¼', 'ç‰¹å¾'
            ],
            IntentType.PREDICTION_REQUEST: [
                'é¢„æµ‹', 'é¢„æŠ¥', 'æ¨æµ‹', 'ä¼°è®¡', 'é¢„ä¼°', 'ä¸‹æœŸ', 'ä¸‹ä¸€æœŸ'
            ],
            IntentType.DATA_QUERY: [
                'æŸ¥è¯¢', 'æŸ¥çœ‹', 'æ˜¾ç¤º', 'æ•°æ®', 'å†å²', 'è®°å½•', 'ç»“æœ'
            ],
            IntentType.HELP_REQUEST: [
                'å¸®åŠ©', 'å¦‚ä½•', 'æ€ä¹ˆ', 'ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'è¯´æ˜', 'æ•™ç¨‹'
            ],
            IntentType.CONFIGURATION: [
                'è®¾ç½®', 'é…ç½®', 'è°ƒæ•´', 'ä¿®æ”¹', 'å‚æ•°', 'é€‰é¡¹', 'åå¥½'
            ],
            IntentType.EXPLANATION: [
                'è§£é‡Š', 'è¯´æ˜', 'åŸç†', 'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•å·¥ä½œ', 'æœºåˆ¶'
            ],
            IntentType.RECOMMENDATION: [
                'æ¨è', 'å»ºè®®', 'æ¨è', 'æœ€å¥½', 'æœ€ä½³', 'ä¼˜åŒ–', 'æ”¹è¿›'
            ],
            IntentType.COMPARISON: [
                'æ¯”è¾ƒ', 'å¯¹æ¯”', 'å·®å¼‚', 'åŒºåˆ«', 'å“ªä¸ªå¥½', 'ä¼˜åŠ£'
            ]
        }
    
    def classify_intent(self, text: str, entities: Dict[str, List[str]]) -> UserIntent:
        """åˆ†ç±»ç”¨æˆ·æ„å›¾"""
        text_lower = text.lower()
        
        # è®¡ç®—æ¯ä¸ªæ„å›¾çš„åŒ¹é…åˆ†æ•°
        intent_scores = {}
        
        for intent_type, keywords in self.intent_patterns.items():
            score = 0
            for keyword in keywords:
                if keyword in text_lower:
                    score += 1
            
            # æ ¹æ®å®ä½“ä¿¡æ¯è°ƒæ•´åˆ†æ•°
            if intent_type == IntentType.PREDICTION_REQUEST and entities['actions']:
                if 'é¢„æµ‹' in entities['actions']:
                    score += 2
            
            if intent_type == IntentType.DATA_QUERY and entities['numbers']:
                score += 1
            
            intent_scores[intent_type] = score
        
        # æ‰¾åˆ°æœ€é«˜åˆ†çš„æ„å›¾
        best_intent = max(intent_scores.items(), key=lambda x: x[1])
        
        if best_intent[1] == 0:
            intent_type = IntentType.UNKNOWN
            confidence = 0.1
        else:
            intent_type = best_intent[0]
            confidence = min(0.9, best_intent[1] / len(self.intent_patterns[intent_type]))
        
        # æå–å‚æ•°
        parameters = self._extract_parameters(text_lower, entities, intent_type)
        
        return UserIntent(
            intent_type=intent_type,
            confidence=confidence,
            entities=entities,
            parameters=parameters,
            context={'original_text': text}
        )
    
    def _extract_parameters(self, text: str, entities: Dict[str, List[str]], 
                          intent_type: IntentType) -> Dict[str, Any]:
        """æå–å‚æ•°"""
        parameters = {}
        
        if intent_type == IntentType.PREDICTION_REQUEST:
            # æå–é¢„æµ‹ç›¸å…³å‚æ•°
            if 'ä¸‹æœŸ' in text or 'ä¸‹ä¸€æœŸ' in text:
                parameters['target'] = 'next_period'
            
            if entities['numbers']:
                parameters['numbers'] = entities['numbers']
            
            if 'åŒè‰²çƒ' in text:
                parameters['lottery_type'] = 'åŒè‰²çƒ'
            elif 'å¤§ä¹é€' in text:
                parameters['lottery_type'] = 'å¤§ä¹é€'
        
        elif intent_type == IntentType.ANALYSIS_REQUEST:
            # æå–åˆ†æç›¸å…³å‚æ•°
            if 'é¢‘ç‡' in text:
                parameters['analysis_type'] = 'frequency'
            elif 'è¶‹åŠ¿' in text:
                parameters['analysis_type'] = 'trend'
            elif 'ç›¸å…³' in text:
                parameters['analysis_type'] = 'correlation'
            
            if entities['numbers']:
                parameters['period_count'] = int(entities['numbers'][0]) if entities['numbers'] else 30
        
        elif intent_type == IntentType.DATA_QUERY:
            # æå–æŸ¥è¯¢ç›¸å…³å‚æ•°
            if entities['periods']:
                parameters['periods'] = entities['periods']
            
            if entities['dates']:
                parameters['dates'] = entities['dates']
        
        return parameters


class KnowledgeBase:
    """çŸ¥è¯†åº“"""
    
    def __init__(self):
        self.knowledge = self._build_knowledge_base()
    
    def _build_knowledge_base(self) -> Dict[str, Any]:
        """æ„å»ºçŸ¥è¯†åº“"""
        return {
            'lottery_info': {
                'åŒè‰²çƒ': {
                    'description': 'åŒè‰²çƒæ˜¯ä¸­å›½ç¦åˆ©å½©ç¥¨çš„ä¸€ç§ï¼Œç”±çº¢çƒå’Œè“çƒç»„æˆ',
                    'red_balls': {'range': '1-33', 'count': 6},
                    'blue_balls': {'range': '1-16', 'count': 1},
                    'draw_frequency': 'æ¯å‘¨äºŒã€å››ã€æ—¥å¼€å¥–'
                },
                'å¤§ä¹é€': {
                    'description': 'å¤§ä¹é€æ˜¯ä¸­å›½ä½“è‚²å½©ç¥¨çš„ä¸€ç§',
                    'front_area': {'range': '1-35', 'count': 5},
                    'back_area': {'range': '1-12', 'count': 2},
                    'draw_frequency': 'æ¯å‘¨ä¸€ã€ä¸‰ã€å…­å¼€å¥–'
                }
            },
            'analysis_methods': {
                'é¢‘ç‡åˆ†æ': 'ç»Ÿè®¡æ¯ä¸ªå·ç åœ¨å†å²å¼€å¥–ä¸­å‡ºç°çš„æ¬¡æ•°',
                'è¶‹åŠ¿åˆ†æ': 'åˆ†æå·ç å‡ºç°çš„æ—¶é—´è¶‹åŠ¿å’Œå‘¨æœŸæ€§',
                'ç›¸å…³æ€§åˆ†æ': 'åˆ†æä¸åŒå·ç ä¹‹é—´çš„å…³è”å…³ç³»',
                'æ¨¡å¼è¯†åˆ«': 'è¯†åˆ«å¼€å¥–å·ç ä¸­çš„ç‰¹å®šæ¨¡å¼'
            },
            'prediction_models': {
                'éšæœºæ£®æ—': 'åŸºäºå†³ç­–æ ‘çš„é›†æˆå­¦ä¹ ç®—æ³•',
                'XGBoost': 'æ¢¯åº¦æå‡å†³ç­–æ ‘ç®—æ³•',
                'LSTM': 'é•¿çŸ­æœŸè®°å¿†ç¥ç»ç½‘ç»œï¼Œé€‚åˆæ—¶åºæ•°æ®',
                'é‡å­ç®—æ³•': 'ä½¿ç”¨é‡å­è®¡ç®—è¿›è¡Œä¼˜åŒ–å’Œé¢„æµ‹'
            },
            'common_questions': {
                'å¦‚ä½•æé«˜é¢„æµ‹å‡†ç¡®ç‡': [
                    'å¢åŠ å†å²æ•°æ®é‡',
                    'ä½¿ç”¨å¤šç§æ¨¡å‹é›†æˆ',
                    'è€ƒè™‘æ›´å¤šç‰¹å¾ç»´åº¦',
                    'å®šæœŸæ›´æ–°æ¨¡å‹å‚æ•°'
                ],
                'å“ªä¸ªæ¨¡å‹æœ€å¥½': [
                    'æ²¡æœ‰ç»å¯¹æœ€å¥½çš„æ¨¡å‹',
                    'å»ºè®®ä½¿ç”¨é›†æˆæ–¹æ³•',
                    'æ ¹æ®æ•°æ®ç‰¹ç‚¹é€‰æ‹©',
                    'å®šæœŸè¯„ä¼°æ¨¡å‹æ€§èƒ½'
                ]
            }
        }
    
    def search_knowledge(self, query: str) -> List[Dict[str, Any]]:
        """æœç´¢çŸ¥è¯†åº“"""
        results = []
        query_lower = query.lower()
        
        # æœç´¢å½©ç¥¨ä¿¡æ¯
        for lottery_type, info in self.knowledge['lottery_info'].items():
            if lottery_type in query_lower or info['description'] in query_lower:
                results.append({
                    'type': 'lottery_info',
                    'title': lottery_type,
                    'content': info,
                    'relevance': 0.9
                })
        
        # æœç´¢åˆ†ææ–¹æ³•
        for method, description in self.knowledge['analysis_methods'].items():
            if any(keyword in query_lower for keyword in method.lower().split()):
                results.append({
                    'type': 'analysis_method',
                    'title': method,
                    'content': description,
                    'relevance': 0.8
                })
        
        # æœç´¢é¢„æµ‹æ¨¡å‹
        for model, description in self.knowledge['prediction_models'].items():
            if model.lower() in query_lower:
                results.append({
                    'type': 'prediction_model',
                    'title': model,
                    'content': description,
                    'relevance': 0.8
                })
        
        # æœç´¢å¸¸è§é—®é¢˜
        for question, answers in self.knowledge['common_questions'].items():
            if any(keyword in query_lower for keyword in question.lower().split()):
                results.append({
                    'type': 'faq',
                    'title': question,
                    'content': answers,
                    'relevance': 0.7
                })
        
        # æŒ‰ç›¸å…³æ€§æ’åº
        results.sort(key=lambda x: x['relevance'], reverse=True)
        
        return results


class ResponseGenerator:
    """å“åº”ç”Ÿæˆå™¨"""
    
    def __init__(self, knowledge_base: KnowledgeBase):
        self.knowledge_base = knowledge_base
        self.response_templates = self._build_response_templates()
    
    def _build_response_templates(self) -> Dict[IntentType, Dict[str, str]]:
        """æ„å»ºå“åº”æ¨¡æ¿"""
        return {
            IntentType.ANALYSIS_REQUEST: {
                'greeting': 'æˆ‘æ¥ä¸ºæ‚¨è¿›è¡Œæ•°æ®åˆ†æã€‚',
                'processing': 'æ­£åœ¨åˆ†æå†å²æ•°æ®...',
                'result': 'åˆ†æç»“æœå¦‚ä¸‹ï¼š',
                'suggestion': 'åŸºäºåˆ†æç»“æœï¼Œæˆ‘å»ºè®®ï¼š'
            },
            IntentType.PREDICTION_REQUEST: {
                'greeting': 'æˆ‘æ¥ä¸ºæ‚¨è¿›è¡Œé¢„æµ‹åˆ†æã€‚',
                'processing': 'æ­£åœ¨è¿è¡Œé¢„æµ‹æ¨¡å‹...',
                'result': 'é¢„æµ‹ç»“æœå¦‚ä¸‹ï¼š',
                'confidence': 'é¢„æµ‹ç½®ä¿¡åº¦ï¼š',
                'warning': 'è¯·æ³¨æ„ï¼šå½©ç¥¨é¢„æµ‹ä»…ä¾›å‚è€ƒï¼Œä¸ä¿è¯å‡†ç¡®æ€§ã€‚'
            },
            IntentType.DATA_QUERY: {
                'greeting': 'æˆ‘æ¥å¸®æ‚¨æŸ¥è¯¢æ•°æ®ã€‚',
                'processing': 'æ­£åœ¨æ£€ç´¢æ•°æ®...',
                'result': 'æŸ¥è¯¢ç»“æœï¼š',
                'empty': 'æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ•°æ®ã€‚'
            },
            IntentType.HELP_REQUEST: {
                'greeting': 'æˆ‘å¾ˆä¹æ„ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚',
                'explanation': 'è®©æˆ‘æ¥è§£é‡Šä¸€ä¸‹ï¼š',
                'guidance': 'æ‚¨å¯ä»¥å°è¯•ï¼š'
            },
            IntentType.EXPLANATION: {
                'greeting': 'è®©æˆ‘æ¥è§£é‡Šè¿™ä¸ªæ¦‚å¿µã€‚',
                'details': 'è¯¦ç»†è¯´æ˜ï¼š',
                'example': 'ä¸¾ä¸ªä¾‹å­ï¼š'
            }
        }
    
    def generate_response(self, intent: UserIntent, 
                         context: Optional[Dict[str, Any]] = None) -> AssistantResponse:
        """ç”Ÿæˆå“åº”"""
        intent_type = intent.intent_type
        
        if intent_type == IntentType.PREDICTION_REQUEST:
            return self._generate_prediction_response(intent, context)
        elif intent_type == IntentType.ANALYSIS_REQUEST:
            return self._generate_analysis_response(intent, context)
        elif intent_type == IntentType.DATA_QUERY:
            return self._generate_query_response(intent, context)
        elif intent_type == IntentType.HELP_REQUEST:
            return self._generate_help_response(intent, context)
        elif intent_type == IntentType.EXPLANATION:
            return self._generate_explanation_response(intent, context)
        else:
            return self._generate_default_response(intent, context)
    
    def _generate_prediction_response(self, intent: UserIntent, 
                                    context: Optional[Dict[str, Any]]) -> AssistantResponse:
        """ç”Ÿæˆé¢„æµ‹å“åº”"""
        templates = self.response_templates[IntentType.PREDICTION_REQUEST]
        
        # æå–å‚æ•°
        lottery_type = intent.parameters.get('lottery_type', 'åŒè‰²çƒ')
        target = intent.parameters.get('target', 'next_period')
        
        # æ¨¡æ‹Ÿé¢„æµ‹ç»“æœ
        if lottery_type == 'åŒè‰²çƒ':
            predicted_red = sorted(np.random.choice(range(1, 34), 6, replace=False).tolist())
            predicted_blue = [np.random.randint(1, 17)]
            confidence = np.random.uniform(0.6, 0.9)
        else:
            predicted_red = sorted(np.random.choice(range(1, 36), 5, replace=False).tolist())
            predicted_blue = sorted(np.random.choice(range(1, 13), 2, replace=False).tolist())
            confidence = np.random.uniform(0.6, 0.9)
        
        content = f"{templates['greeting']}\n\n"
        content += f"{templates['result']}\n"
        content += f"å½©ç¥¨ç±»å‹ï¼š{lottery_type}\n"
        
        if lottery_type == 'åŒè‰²çƒ':
            content += f"é¢„æµ‹çº¢çƒï¼š{' '.join(map(str, predicted_red))}\n"
            content += f"é¢„æµ‹è“çƒï¼š{predicted_blue[0]}\n"
        else:
            content += f"å‰åŒºå·ç ï¼š{' '.join(map(str, predicted_red))}\n"
            content += f"ååŒºå·ç ï¼š{' '.join(map(str, predicted_blue))}\n"
        
        content += f"\n{templates['confidence']}{confidence:.1%}\n"
        content += f"\nâš ï¸ {templates['warning']}"
        
        return AssistantResponse(
            response_type=ResponseType.MIXED,
            content=content,
            data={
                'lottery_type': lottery_type,
                'predicted_numbers': {
                    'red': predicted_red,
                    'blue': predicted_blue
                },
                'confidence': confidence
            },
            confidence=confidence,
            metadata={'intent_type': intent.intent_type.value}
        )
    
    def _generate_analysis_response(self, intent: UserIntent, 
                                  context: Optional[Dict[str, Any]]) -> AssistantResponse:
        """ç”Ÿæˆåˆ†æå“åº”"""
        templates = self.response_templates[IntentType.ANALYSIS_REQUEST]
        
        analysis_type = intent.parameters.get('analysis_type', 'frequency')
        period_count = intent.parameters.get('period_count', 30)
        
        content = f"{templates['greeting']}\n\n"
        content += f"åˆ†æç±»å‹ï¼š{analysis_type}\n"
        content += f"åˆ†ææœŸæ•°ï¼šæœ€è¿‘{period_count}æœŸ\n\n"
        
        # æ¨¡æ‹Ÿåˆ†æç»“æœ
        if analysis_type == 'frequency':
            content += "ğŸ“Š å·ç é¢‘ç‡åˆ†æç»“æœï¼š\n"
            content += "çƒ­é—¨å·ç ï¼š1, 7, 12, 18, 25, 33\n"
            content += "å†·é—¨å·ç ï¼š3, 9, 15, 21, 27, 31\n"
            content += "å¹³å‡å‡ºç°é¢‘ç‡ï¼š16.7%\n"
        elif analysis_type == 'trend':
            content += "ğŸ“ˆ è¶‹åŠ¿åˆ†æç»“æœï¼š\n"
            content += "æ•´ä½“è¶‹åŠ¿ï¼šå·ç åˆ†å¸ƒè¶‹äºå‡åŒ€\n"
            content += "å‘¨æœŸæ€§ï¼šå­˜åœ¨7æœŸçš„å°å‘¨æœŸ\n"
            content += "æ³¢åŠ¨æ€§ï¼šä¸­ç­‰æ³¢åŠ¨\n"
        else:
            content += "ğŸ” ç»¼åˆåˆ†æç»“æœï¼š\n"
            content += "æ•°æ®è´¨é‡ï¼šè‰¯å¥½\n"
            content += "æ¨¡å¼è¯†åˆ«ï¼šå‘ç°3ç§å¸¸è§æ¨¡å¼\n"
            content += "é¢„æµ‹éš¾åº¦ï¼šä¸­ç­‰\n"
        
        content += f"\n{templates['suggestion']}\n"
        content += "1. ç»“åˆå¤šç§åˆ†ææ–¹æ³•\n"
        content += "2. å…³æ³¨é•¿æœŸè¶‹åŠ¿å˜åŒ–\n"
        content += "3. å®šæœŸæ›´æ–°åˆ†ææ•°æ®"
        
        return AssistantResponse(
            response_type=ResponseType.MIXED,
            content=content,
            data={
                'analysis_type': analysis_type,
                'period_count': period_count,
                'results': {
                    'hot_numbers': [1, 7, 12, 18, 25, 33],
                    'cold_numbers': [3, 9, 15, 21, 27, 31]
                }
            },
            confidence=0.8,
            metadata={'intent_type': intent.intent_type.value}
        )
    
    def _generate_query_response(self, intent: UserIntent, 
                               context: Optional[Dict[str, Any]]) -> AssistantResponse:
        """ç”ŸæˆæŸ¥è¯¢å“åº”"""
        templates = self.response_templates[IntentType.DATA_QUERY]
        
        content = f"{templates['greeting']}\n\n"
        
        if intent.entities['periods']:
            periods = intent.entities['periods']
            content += f"æŸ¥è¯¢æœŸå·ï¼š{', '.join(periods)}\n"
            content += "æŸ¥è¯¢ç»“æœï¼š\n"
            for period in periods[:3]:  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                content += f"æœŸå· {period}ï¼šçº¢çƒ 01 07 12 18 25 33ï¼Œè“çƒ 08\n"
        elif intent.entities['dates']:
            dates = intent.entities['dates']
            content += f"æŸ¥è¯¢æ—¥æœŸï¼š{', '.join(dates)}\n"
            content += "åœ¨æ­¤æ—¥æœŸèŒƒå›´å†…å…±æ‰¾åˆ° 5 æœŸå¼€å¥–è®°å½•\n"
        else:
            content += "æœ€è¿‘å¼€å¥–è®°å½•ï¼š\n"
            content += "2024001æœŸï¼šçº¢çƒ 03 08 15 22 29 32ï¼Œè“çƒ 12\n"
            content += "2024002æœŸï¼šçº¢çƒ 05 11 17 24 28 31ï¼Œè“çƒ 06\n"
            content += "2024003æœŸï¼šçº¢çƒ 02 09 16 21 26 33ï¼Œè“çƒ 14\n"
        
        return AssistantResponse(
            response_type=ResponseType.DATA,
            content=content,
            data={
                'query_type': 'historical_data',
                'results': [
                    {'period': '2024001', 'red': [3, 8, 15, 22, 29, 32], 'blue': [12]},
                    {'period': '2024002', 'red': [5, 11, 17, 24, 28, 31], 'blue': [6]},
                    {'period': '2024003', 'red': [2, 9, 16, 21, 26, 33], 'blue': [14]}
                ]
            },
            confidence=0.9,
            metadata={'intent_type': intent.intent_type.value}
        )
    
    def _generate_help_response(self, intent: UserIntent, 
                              context: Optional[Dict[str, Any]]) -> AssistantResponse:
        """ç”Ÿæˆå¸®åŠ©å“åº”"""
        templates = self.response_templates[IntentType.HELP_REQUEST]
        
        # æœç´¢çŸ¥è¯†åº“
        query = intent.context.get('original_text', '')
        knowledge_results = self.knowledge_base.search_knowledge(query)
        
        content = f"{templates['greeting']}\n\n"
        
        if knowledge_results:
            content += f"{templates['explanation']}\n"
            for result in knowledge_results[:3]:  # æ˜¾ç¤ºå‰3ä¸ªç»“æœ
                content += f"\nğŸ“Œ {result['title']}ï¼š\n"
                if isinstance(result['content'], dict):
                    for key, value in result['content'].items():
                        content += f"  â€¢ {key}ï¼š{value}\n"
                elif isinstance(result['content'], list):
                    for item in result['content']:
                        content += f"  â€¢ {item}\n"
                else:
                    content += f"  {result['content']}\n"
        else:
            content += "æˆ‘å¯ä»¥å¸®æ‚¨ï¼š\n"
            content += "â€¢ ğŸ¯ é¢„æµ‹ä¸‹æœŸå·ç \n"
            content += "â€¢ ğŸ“Š åˆ†æå†å²æ•°æ®\n"
            content += "â€¢ ğŸ” æŸ¥è¯¢å¼€å¥–è®°å½•\n"
            content += "â€¢ âš™ï¸ é…ç½®ç³»ç»Ÿå‚æ•°\n"
            content += "â€¢ ğŸ“ˆ ç”Ÿæˆç»Ÿè®¡å›¾è¡¨\n"
            content += "â€¢ ğŸ’¡ æä¾›ä¸“ä¸šå»ºè®®\n"
        
        content += f"\n{templates['guidance']}\n"
        content += "â€¢ è¯´\"é¢„æµ‹åŒè‰²çƒä¸‹æœŸå·ç \"\n"
        content += "â€¢ è¯´\"åˆ†ææœ€è¿‘30æœŸçš„é¢‘ç‡\"\n"
        content += "â€¢ è¯´\"æŸ¥è¯¢2024001æœŸå¼€å¥–ç»“æœ\"\n"
        
        return AssistantResponse(
            response_type=ResponseType.TEXT,
            content=content,
            confidence=0.9,
            metadata={'intent_type': intent.intent_type.value}
        )
    
    def _generate_explanation_response(self, intent: UserIntent, 
                                     context: Optional[Dict[str, Any]]) -> AssistantResponse:
        """ç”Ÿæˆè§£é‡Šå“åº”"""
        templates = self.response_templates[IntentType.EXPLANATION]
        
        content = f"{templates['greeting']}\n\n"
        
        # æ ¹æ®å®ä½“æä¾›è§£é‡Š
        if 'éšæœºæ£®æ—' in intent.context.get('original_text', ''):
            content += f"{templates['details']}\n"
            content += "éšæœºæ£®æ—æ˜¯ä¸€ç§é›†æˆå­¦ä¹ ç®—æ³•ï¼Œå®ƒçš„å·¥ä½œåŸç†æ˜¯ï¼š\n"
            content += "1. ğŸŒ³ æ„å»ºå¤šä¸ªå†³ç­–æ ‘\n"
            content += "2. ğŸ² æ¯ä¸ªæ ‘ä½¿ç”¨ä¸åŒçš„æ•°æ®å­é›†\n"
            content += "3. ğŸ—³ï¸ é€šè¿‡æŠ•ç¥¨å†³å®šæœ€ç»ˆç»“æœ\n"
            content += "4. ğŸ“Š æä¾›ç‰¹å¾é‡è¦æ€§åˆ†æ\n\n"
            content += f"{templates['example']}\n"
            content += "åœ¨å½©ç¥¨é¢„æµ‹ä¸­ï¼Œéšæœºæ£®æ—å¯ä»¥ï¼š\n"
            content += "â€¢ åˆ†æå·ç å‡ºç°çš„å†å²æ¨¡å¼\n"
            content += "â€¢ è¯†åˆ«é‡è¦çš„é¢„æµ‹ç‰¹å¾\n"
            content += "â€¢ æä¾›ç¨³å®šçš„é¢„æµ‹ç»“æœ\n"
        elif 'é‡å­' in intent.context.get('original_text', ''):
            content += f"{templates['details']}\n"
            content += "é‡å­è®¡ç®—åœ¨å½©ç¥¨åˆ†æä¸­çš„åº”ç”¨ï¼š\n"
            content += "1. âš›ï¸ é‡å­å åŠ ï¼šåŒæ—¶å¤„ç†å¤šç§å¯èƒ½æ€§\n"
            content += "2. ğŸ”— é‡å­çº ç¼ ï¼šå‘ç°å¤æ‚çš„å…³è”å…³ç³»\n"
            content += "3. ğŸš€ é‡å­åŠ é€Ÿï¼šè§£å†³å¤§è§„æ¨¡ä¼˜åŒ–é—®é¢˜\n"
            content += "4. ğŸ¯ é‡å­ç®—æ³•ï¼šQAOAã€VQEç­‰ä¸“é—¨ç®—æ³•\n"
        else:
            content += "è¯·å‘Šè¯‰æˆ‘æ‚¨æƒ³äº†è§£ä»€ä¹ˆæ¦‚å¿µï¼Œæˆ‘ä¼šè¯¦ç»†è§£é‡Šã€‚\n"
            content += "æˆ‘å¯ä»¥è§£é‡Šçš„å†…å®¹åŒ…æ‹¬ï¼š\n"
            content += "â€¢ ğŸ¤– æœºå™¨å­¦ä¹ ç®—æ³•\n"
            content += "â€¢ ğŸ“Š æ•°æ®åˆ†ææ–¹æ³•\n"
            content += "â€¢ âš›ï¸ é‡å­è®¡ç®—åŸç†\n"
            content += "â€¢ ğŸ“ˆ ç»Ÿè®¡å­¦æ¦‚å¿µ\n"
        
        return AssistantResponse(
            response_type=ResponseType.TEXT,
            content=content,
            confidence=0.8,
            metadata={'intent_type': intent.intent_type.value}
        )
    
    def _generate_default_response(self, intent: UserIntent, 
                                 context: Optional[Dict[str, Any]]) -> AssistantResponse:
        """ç”Ÿæˆé»˜è®¤å“åº”"""
        content = "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰å®Œå…¨ç†è§£æ‚¨çš„é—®é¢˜ã€‚\n\n"
        content += "æ‚¨å¯ä»¥å°è¯•è¿™æ ·é—®æˆ‘ï¼š\n"
        content += "â€¢ \"é¢„æµ‹åŒè‰²çƒä¸‹æœŸå·ç \"\n"
        content += "â€¢ \"åˆ†ææœ€è¿‘50æœŸçš„å·ç é¢‘ç‡\"\n"
        content += "â€¢ \"æŸ¥è¯¢2024001æœŸçš„å¼€å¥–ç»“æœ\"\n"
        content += "â€¢ \"è§£é‡Šä»€ä¹ˆæ˜¯éšæœºæ£®æ—ç®—æ³•\"\n"
        content += "â€¢ \"å¦‚ä½•æé«˜é¢„æµ‹å‡†ç¡®ç‡\"\n\n"
        content += "å¦‚æœæ‚¨éœ€è¦æ›´å¤šå¸®åŠ©ï¼Œè¯·è¯´\"å¸®åŠ©\"ã€‚"
        
        return AssistantResponse(
            response_type=ResponseType.TEXT,
            content=content,
            confidence=0.5,
            metadata={'intent_type': intent.intent_type.value}
        )


class ConversationManager:
    """å¯¹è¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.conversations = {}  # session_id -> messages
        self.max_history = 50    # æœ€å¤§å†å²è®°å½•æ•°
    
    def add_message(self, session_id: str, message: ConversationMessage):
        """æ·»åŠ æ¶ˆæ¯"""
        if session_id not in self.conversations:
            self.conversations[session_id] = []
        
        self.conversations[session_id].append(message)
        
        # é™åˆ¶å†å²è®°å½•é•¿åº¦
        if len(self.conversations[session_id]) > self.max_history:
            self.conversations[session_id] = self.conversations[session_id][-self.max_history:]
    
    def get_conversation_history(self, session_id: str, limit: int = 10) -> List[ConversationMessage]:
        """è·å–å¯¹è¯å†å²"""
        if session_id not in self.conversations:
            return []
        
        return self.conversations[session_id][-limit:]
    
    def get_context(self, session_id: str) -> Dict[str, Any]:
        """è·å–å¯¹è¯ä¸Šä¸‹æ–‡"""
        history = self.get_conversation_history(session_id, limit=5)
        
        context = {
            'previous_intents': [],
            'mentioned_entities': set(),
            'conversation_length': len(history),
            'last_response_time': None
        }
        
        for message in history:
            if message.metadata:
                if 'intent_type' in message.metadata:
                    context['previous_intents'].append(message.metadata['intent_type'])
                
                if 'entities' in message.metadata:
                    for entity_list in message.metadata['entities'].values():
                        context['mentioned_entities'].update(entity_list)
            
            context['last_response_time'] = message.timestamp
        
        context['mentioned_entities'] = list(context['mentioned_entities'])
        
        return context


class IntelligentAssistant:
    """æ™ºèƒ½åŠ©æ‰‹ä¸»ç±»"""
    
    def __init__(self):
        self.nlp_processor = NLPProcessor()
        self.intent_classifier = IntentClassifier()
        self.knowledge_base = KnowledgeBase()
        self.response_generator = ResponseGenerator(self.knowledge_base)
        self.conversation_manager = ConversationManager()
        
        logger.info("AIæ™ºèƒ½åŠ©æ‰‹åˆå§‹åŒ–å®Œæˆ")
    
    def process_message(self, user_message: str, session_id: str = "default") -> AssistantResponse:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
        start_time = time.time()
        
        # 1. é¢„å¤„ç†æ–‡æœ¬
        processed_text = self.nlp_processor.preprocess_text(user_message)
        
        # 2. æå–å®ä½“
        entities = self.nlp_processor.extract_entities(user_message)
        
        # 3. åˆ†ç±»æ„å›¾
        intent = self.intent_classifier.classify_intent(user_message, entities)
        
        # 4. è·å–å¯¹è¯ä¸Šä¸‹æ–‡
        context = self.conversation_manager.get_context(session_id)
        
        # 5. ç”Ÿæˆå“åº”
        response = self.response_generator.generate_response(intent, context)
        
        # 6. è®°å½•å¯¹è¯
        user_msg = ConversationMessage(
            role=ConversationRole.USER,
            content=user_message,
            timestamp=time.time(),
            metadata={
                'entities': entities,
                'intent_type': intent.intent_type.value,
                'intent_confidence': intent.confidence
            }
        )
        
        assistant_msg = ConversationMessage(
            role=ConversationRole.ASSISTANT,
            content=response.content,
            timestamp=time.time(),
            metadata={
                'response_type': response.response_type.value,
                'confidence': response.confidence,
                'processing_time': time.time() - start_time
            }
        )
        
        self.conversation_manager.add_message(session_id, user_msg)
        self.conversation_manager.add_message(session_id, assistant_msg)
        
        return response
    
    def get_conversation_history(self, session_id: str = "default") -> List[Dict[str, Any]]:
        """è·å–å¯¹è¯å†å²"""
        messages = self.conversation_manager.get_conversation_history(session_id)
        return [msg.to_dict() for msg in messages]
    
    def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """åˆ†ææƒ…æ„Ÿ"""
        return self.nlp_processor.analyze_sentiment(text)
    
    def get_system_capabilities(self) -> Dict[str, Any]:
        """è·å–ç³»ç»Ÿèƒ½åŠ›"""
        return {
            'nlp_capabilities': {
                'text_preprocessing': True,
                'entity_extraction': True,
                'sentiment_analysis': True,
                'intent_classification': True
            },
            'supported_intents': [intent.value for intent in IntentType],
            'knowledge_domains': list(self.knowledge_base.knowledge.keys()),
            'conversation_management': True,
            'multilingual_support': False,  # ç›®å‰ä¸»è¦æ”¯æŒä¸­æ–‡
            'available_models': {
                'nltk': NLTK_AVAILABLE,
                'spacy': SPACY_AVAILABLE,
                'transformers': TRANSFORMERS_AVAILABLE,
                'openai': OPENAI_AVAILABLE
            }
        }


# å…¨å±€åŠ©æ‰‹å®ä¾‹
_intelligent_assistant = None

def get_intelligent_assistant() -> IntelligentAssistant:
    """è·å–æ™ºèƒ½åŠ©æ‰‹å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _intelligent_assistant
    if _intelligent_assistant is None:
        _intelligent_assistant = IntelligentAssistant()
    return _intelligent_assistant


def main():
    """æµ‹è¯•ä¸»å‡½æ•°"""
    print("ğŸ¤– æµ‹è¯•AIæ™ºèƒ½åŠ©æ‰‹ç³»ç»Ÿ...")
    
    # åˆ›å»ºåŠ©æ‰‹å®ä¾‹
    assistant = get_intelligent_assistant()
    
    # è·å–ç³»ç»Ÿèƒ½åŠ›
    capabilities = assistant.get_system_capabilities()
    print(f"ç³»ç»Ÿèƒ½åŠ›: {json.dumps(capabilities, indent=2, ensure_ascii=False)}")
    
    # æµ‹è¯•å¯¹è¯
    test_messages = [
        "ä½ å¥½ï¼Œæˆ‘æƒ³é¢„æµ‹åŒè‰²çƒä¸‹æœŸå·ç ",
        "åˆ†ææœ€è¿‘30æœŸçš„å·ç é¢‘ç‡",
        "æŸ¥è¯¢2024001æœŸçš„å¼€å¥–ç»“æœ",
        "ä»€ä¹ˆæ˜¯éšæœºæ£®æ—ç®—æ³•ï¼Ÿ",
        "å¦‚ä½•æé«˜é¢„æµ‹å‡†ç¡®ç‡ï¼Ÿ",
        "å¸®åŠ©æˆ‘é…ç½®ç³»ç»Ÿå‚æ•°"
    ]
    
    session_id = "test_session"
    
    print(f"\nå¼€å§‹å¯¹è¯æµ‹è¯•ï¼ˆä¼šè¯IDï¼š{session_id}ï¼‰ï¼š")
    print("=" * 60)
    
    for i, message in enumerate(test_messages, 1):
        print(f"\nğŸ‘¤ ç”¨æˆ·: {message}")
        
        try:
            response = assistant.process_message(message, session_id)
            print(f"ğŸ¤– åŠ©æ‰‹: {response.content}")
            
            if response.data:
                print(f"ğŸ“Š æ•°æ®: {json.dumps(response.data, indent=2, ensure_ascii=False)}")
            
            print(f"ğŸ¯ ç½®ä¿¡åº¦: {response.confidence:.2%}")
            print(f"ğŸ“ å“åº”ç±»å‹: {response.response_type.value}")
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
        
        print("-" * 40)
    
    # æµ‹è¯•æƒ…æ„Ÿåˆ†æ
    print(f"\næƒ…æ„Ÿåˆ†ææµ‹è¯•:")
    test_texts = [
        "è¿™ä¸ªé¢„æµ‹ç»“æœå¾ˆå¥½ï¼Œæˆ‘å¾ˆæ»¡æ„",
        "é¢„æµ‹ä¸å‡†ç¡®ï¼Œå¾ˆå¤±æœ›",
        "ç³»ç»Ÿè¿è¡Œæ­£å¸¸"
    ]
    
    for text in test_texts:
        sentiment = assistant.analyze_sentiment(text)
        print(f"æ–‡æœ¬: {text}")
        print(f"æƒ…æ„Ÿ: {sentiment['sentiment']}, ç½®ä¿¡åº¦: {sentiment['confidence']:.2%}")
    
    # è·å–å¯¹è¯å†å²
    print(f"\nå¯¹è¯å†å²:")
    history = assistant.get_conversation_history(session_id)
    for msg in history[-4:]:  # æ˜¾ç¤ºæœ€å4æ¡æ¶ˆæ¯
        role_emoji = "ğŸ‘¤" if msg['role'] == 'user' else "ğŸ¤–"
        print(f"{role_emoji} {msg['role']}: {msg['content'][:50]}...")
    
    print(f"\nâœ… AIæ™ºèƒ½åŠ©æ‰‹ç³»ç»Ÿæµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“Š å¤„ç†äº† {len(test_messages)} æ¡æ¶ˆæ¯")
    print(f"ğŸ’¬ å¯¹è¯å†å²åŒ…å« {len(history)} æ¡è®°å½•")


if __name__ == "__main__":
    main()
